\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{XuH2015,Rodgers2006,Broderick2008,ferris2007materials,Kalidindi2011}
\citation{SergeiV.Kalinin2015}
\citation{SergeiV.Kalinin2015}
\citation{DeCost2015}
\citation{DeCost2015}
\citation{SergeiV.Kalinin2015}
\citation{SergeiV.Kalinin2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {3.}IMAGE DRIVEN MACHINE LEARNING METHODS FOR MICROSTRUCTURE RECOGNITION}{16}{chapter.3}}
\newlabel{chap:COMMAT}{{3}{16}{IMAGE DRIVEN MACHINE LEARNING METHODS FOR MICROSTRUCTURE RECOGNITION}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{16}{section.3.1}}
\newlabel{intro}{{3.1}{16}{Introduction}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Reducing classification error by optimizing the components of the image classification pipeline together instead of in isolation. The steps in the image classification pipeline highlighted in red are optimized simultaneously to minimize the classification error.\relax }}{17}{figure.caption.18}}
\newlabel{fig:chapter3}{{3.1}{17}{Reducing classification error by optimizing the components of the image classification pipeline together instead of in isolation. The steps in the image classification pipeline highlighted in red are optimized simultaneously to minimize the classification error.\relax }{figure.caption.18}{}}
\citation{Schaefer2005}
\citation{Mao2014}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Alloy Fabrication and Sample Preparation}{18}{section.3.2}}
\newlabel{sample_prep}{{3.2}{18}{Alloy Fabrication and Sample Preparation}{section.3.2}{}}
\citation{Barber2003}
\citation{Barber2003}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Image Data Sets}{19}{section.3.3}}
\newlabel{data_sets}{{3.3}{19}{Image Data Sets}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Examples of micrographs used in classification. Micrographs shown in (a) and (b) are longitudinal and transverse cross-sectional views of dendrites, whereas micrographs in (c) do not contain dendrites. Micrographs in (a), (b) and (c) were used in Task 1, and micrographs in (a) and (b) were used in Task 2.\relax }}{20}{figure.caption.19}}
\newlabel{fig:example_images}{{3.2}{20}{Examples of micrographs used in classification. Micrographs shown in (a) and (b) are longitudinal and transverse cross-sectional views of dendrites, whereas micrographs in (c) do not contain dendrites. Micrographs in (a), (b) and (c) were used in Task 1, and micrographs in (a) and (b) were used in Task 2.\relax }{figure.caption.19}{}}
\citation{Keogh2010}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Approach}{21}{section.3.4}}
\newlabel{approach}{{3.4}{21}{Approach}{section.3.4}{}}
\citation{Haralick1973}
\citation{Dalal2005}
\citation{Ojala2002}
\citation{Hu1962,Otsu1975}
\citation{Khotanzad1990}
\citation{Lievers2004}
\citation{Yang2007,Bay2006}
\citation{Jia2014}
\citation{Krizhevsky2012}
\citation{Jia2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Overview of approach used in classification of micrograph data. The approach summarized here shows 140 different combinations of feature extraction, feature selection and classification methods completed. The same approach presented here was completed first for Task 1 (Data Set 1), then for Task 2 (Data Set 2).\relax }}{22}{figure.caption.20}}
\newlabel{fig:approach_overview}{{3.3}{22}{Overview of approach used in classification of micrograph data. The approach summarized here shows 140 different combinations of feature extraction, feature selection and classification methods completed. The same approach presented here was completed first for Task 1 (Data Set 1), then for Task 2 (Data Set 2).\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Computer Vision and Machine Learning Methods}{22}{section.3.5}}
\newlabel{methods}{{3.5}{22}{Computer Vision and Machine Learning Methods}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Feature extraction}{22}{subsection.3.5.1}}
\newlabel{feature_extraction}{{3.5.1}{22}{Feature extraction}{subsection.3.5.1}{}}
\citation{Haralick1973}
\citation{Dalal2005}
\citation{Ojala2002}
\citation{Hu1962,Otsu1975}
\citation{Otsu1975}
\citation{Khotanzad1990}
\citation{Lievers2004}
\citation{Klinger2012}
\citation{Yang2007,Bay2006}
\citation{Bay2006}
\citation{curse}
\citation{pca}
\citation{anova}
\citation{chi}
\citation{fisher}
\citation{gini}
\citation{pca}
\citation{anova}
\citation{gini}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Dimensionality reduction}{27}{subsection.3.5.2}}
\newlabel{dimensionality_reduction}{{3.5.2}{27}{Dimensionality reduction}{subsection.3.5.2}{}}
\citation{svm}
\citation{rf}
\citation{nn}
\citation{voting}
\citation{nb}
\citation{Kotsiantis2007}
\citation{svm}
\citation{nn}
\citation{rf}
\citation{voting}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Classification}{28}{subsection.3.5.3}}
\newlabel{classification}{{3.5.3}{28}{Classification}{subsection.3.5.3}{}}
\citation{Pattan2010,Prakash2011,Kumara,MacSleyne2008,Sluytman2012}
\citation{MacSleyne2008}
\citation{Pattan2010,Prakash2011,Kumara}
\citation{Sluytman2012}
\citation{DeCost2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Related Work in Materials Science}{30}{subsection.3.5.4}}
\newlabel{related_work}{{3.5.4}{30}{Related Work in Materials Science}{subsection.3.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Selection of Model Parameters}{31}{section.3.6}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Software and Hardware Specifications}{31}{section.3.7}}
\citation{Jia2014}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Results and Discussion}{32}{section.3.8}}
\newlabel{results}{{3.8}{32}{Results and Discussion}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Task 1 - Dendritic versus non-dendritic microstructures}{32}{subsection.3.8.1}}
\newlabel{Task1}{{3.8.1}{32}{Task 1 - Dendritic versus non-dendritic microstructures}{subsection.3.8.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Feature extraction methods and corresponding classification accuracies (in \%), that contributed to the maximum classification accuracy for each combination of feature selection and classification method tested for Task 1.\relax }}{33}{table.3.1}}
\newlabel{tab:FE_Task1}{{3.1}{33}{Feature extraction methods and corresponding classification accuracies (in \%), that contributed to the maximum classification accuracy for each combination of feature selection and classification method tested for Task 1.\relax }{table.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Feature selection methods and corresponding classification accuracies (in \%), that contributed to the maximum classification accuracy for each combination of feature extraction and classification method tested for Task 1.\relax }}{34}{table.3.2}}
\newlabel{tab:FS_Task1}{{3.2}{34}{Feature selection methods and corresponding classification accuracies (in \%), that contributed to the maximum classification accuracy for each combination of feature extraction and classification method tested for Task 1.\relax }{table.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Classification methods that contributed to the maximum classification accuracy (in \%) for each combination of feature extraction and feature selection method tested for Task 1.\relax }}{35}{table.3.3}}
\newlabel{tab:Classifier_Task1}{{3.3}{35}{Classification methods that contributed to the maximum classification accuracy (in \%) for each combination of feature extraction and feature selection method tested for Task 1.\relax }{table.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Top 20 configurations of algorithms in Task 1 with error bars representing one standard deviation. There is no significant difference in the accuracies in the different configuations. Most of the feature extraction algorithms in the top 20 configurations are pre-trained CNNs (\textit  {caffe-fc6} or \textit  {caffe-conv5})\relax }}{36}{figure.caption.21}}
\newlabel{fig:error_bars1}{{3.4}{36}{Top 20 configurations of algorithms in Task 1 with error bars representing one standard deviation. There is no significant difference in the accuracies in the different configuations. Most of the feature extraction algorithms in the top 20 configurations are pre-trained CNNs (\textit {caffe-fc6} or \textit {caffe-conv5})\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Task 2 - Longitudinal versus transverse cross-sectional views of dendrites}{36}{subsection.3.8.2}}
\newlabel{Task2}{{3.8.2}{36}{Task 2 - Longitudinal versus transverse cross-sectional views of dendrites}{subsection.3.8.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Average rank of the algorithms in Task 1 with respect to feature extraction, dimensionality reduction and classification. The average rank of an algorithm quantifies it's position in the sorted list of configurations.\relax }}{37}{table.3.4}}
\newlabel{table:average_rank1}{{3.4}{37}{Average rank of the algorithms in Task 1 with respect to feature extraction, dimensionality reduction and classification. The average rank of an algorithm quantifies it's position in the sorted list of configurations.\relax }{table.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Feature extraction methods and corresponding classification accuracies (in \%), that contributed to the maximum classification accuracy for each combination of feature selection and classification method tested for Task 2.\relax }}{38}{table.3.5}}
\newlabel{tab:FE_Task2}{{3.5}{38}{Feature extraction methods and corresponding classification accuracies (in \%), that contributed to the maximum classification accuracy for each combination of feature selection and classification method tested for Task 2.\relax }{table.3.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Feature selection methods, and corresponding classification accuracies (in \%), that contributed to the maximum classification accuracy for each combination of feature extraction and classification method tested for Task 2.\relax }}{38}{table.3.6}}
\newlabel{tab:FS_Task2}{{3.6}{38}{Feature selection methods, and corresponding classification accuracies (in \%), that contributed to the maximum classification accuracy for each combination of feature extraction and classification method tested for Task 2.\relax }{table.3.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Classification models that yielded the maximum classification accuracy (in \%) for each combination of feature extraction and feature selection method tested for Task 2.\relax }}{39}{table.3.7}}
\newlabel{tab:Classifier_Task2}{{3.7}{39}{Classification models that yielded the maximum classification accuracy (in \%) for each combination of feature extraction and feature selection method tested for Task 2.\relax }{table.3.7}{}}
\citation{Krizhevsky2012}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Top 20 configurations of algorithms in Task 2 with error bars representing one standard deviation. There is no significant difference in the accuracies in the different configuations. Most of the feature extraction algorithms in the top 20 configurations are pre-trained CNNs (\textit  {caffe-fc6} or \textit  {caffe-conv5})\relax }}{40}{figure.caption.22}}
\newlabel{fig:error_bars2}{{3.5}{40}{Top 20 configurations of algorithms in Task 2 with error bars representing one standard deviation. There is no significant difference in the accuracies in the different configuations. Most of the feature extraction algorithms in the top 20 configurations are pre-trained CNNs (\textit {caffe-fc6} or \textit {caffe-conv5})\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Limitations}{40}{section.3.9}}
\newlabel{limitations}{{3.9}{40}{Limitations}{section.3.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces Average rank of the algorithms in Task 2 with respect to feature extraction, dimensionality reduction and classification. The average rank of an algorithm quantifies it's position in the sorted list of configurations.\relax }}{41}{table.3.8}}
\newlabel{table:average_rank2}{{3.8}{41}{Average rank of the algorithms in Task 2 with respect to feature extraction, dimensionality reduction and classification. The average rank of an algorithm quantifies it's position in the sorted list of configurations.\relax }{table.3.8}{}}
\citation{DeCost2015,Impoco2015,XuH2015,Bostanabad2016,Taffese2015}
\citation{Guo2016}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Conclusions}{42}{section.3.10}}
\newlabel{conclusions}{{3.10}{42}{Conclusions}{section.3.10}{}}
\@setckpt{COMMAT}{
\setcounter{page}{44}
\setcounter{equation}{9}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{10}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{8}
\setcounter{firstchapter}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{parentequation}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{34}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{algorithm}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{prop}{0}
\setcounter{rmk}{0}
\setcounter{section@level}{1}
\setcounter{lstlisting}{0}
}
