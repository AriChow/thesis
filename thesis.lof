\contentsline {figure}{\numberline {1.1}{\ignorespaces Representation of an image classification pipeline\relax }}{2}{figure.caption.10}
\contentsline {figure}{\numberline {2.1}{\ignorespaces Examples of micrographs used in classification. Micrographs shown in (a) and (b) are longitudinal and transverse cross-sectional views of dendrites, whereas micrographs in (c) do not contain dendrites. Micrographs in (a), (b) and (c) were used in Task 1, and micrographs in (a) and (b) were used in Task 2.\relax }}{9}{figure.caption.11}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Overview of approach used in classification of micrograph data. The approach summarized here shows 140 different combinations of feature extraction, feature selection and classification methods completed. The same approach presented here was completed first for Task 1 (Data Set 1), then for Task 2 (Data Set 2).\relax }}{10}{figure.caption.12}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Flowchart of microstructure recognition tasks, referred to here as Task 1 and Task 2. Tasks 1 and 2 are both binary classification tasks. Task 1 involves distinguishing between micrographs that depict dendritic morphologies from those that do not contain this particular microstructural feature. From micrographs identified as containing dendrites (completed in Task 1), Task 2 involves distinguishing between different cross-sectional views (longitudinal or transverse) of dendritic microstructures.\relax }}{11}{figure.caption.13}
\contentsline {figure}{\numberline {3.1}{\ignorespaces Depiction of the different morphologies in the natural data with respect to a multichannel image, overlaid with different protein markers. The three types of morphologies analyzed in this study is represented on the right.\relax }}{33}{figure.caption.15}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Development of the 3D virtual model\relax }}{35}{figure.caption.16}
\contentsline {figure}{\numberline {3.3}{\ignorespaces 3D virtual models and their corresponding projections along different planes of view (a) Linear model of RoundLumen- (b) Linear model of RoundLumen+ (c) Non-linear model of RoundLumen+ (d) Non-linear model of Twins\relax }}{35}{figure.caption.17}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Examples of vessel classes RoundLumen- (a/d), RoundLumen+ (b/e) and Twins (c/f) for natural (a/b/c) and virtual data (d/e/f)\relax }}{38}{figure.caption.18}
\contentsline {figure}{\numberline {3.5}{\ignorespaces ROC curves for classification: (a) \textit {RoundLumen} and \textit {Twins} and (b) \textit {RoundLumen-} and \textit {RoundLumen+} .\relax }}{39}{figure.caption.19}
\contentsline {figure}{\numberline {4.1}{\ignorespaces Examples of a single colon tumor tissue sample stained with (a) E\_cad (b) pck26 (c) CK15 (d) Vimentin\relax }}{42}{figure.caption.20}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Preliminary noise reduction on an image from CK15 (a) Original image (b) Noise-reduced image \relax }}{43}{figure.caption.21}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Images from (a) E\_cad, (b) pck26 and (c) CK15 are used to form the (d) Combined image\relax }}{44}{figure.caption.22}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Three sample images from E\_cad stained tissues\relax }}{47}{figure.caption.23}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Discrete distributions of QoI scores for (a) E\_cad (b) pck26 (c) CK15\relax }}{47}{figure.caption.24}
\contentsline {figure}{\numberline {5.1}{\ignorespaces Representation of a machine learning pipeline\relax }}{49}{figure.caption.25}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Representation of an image classification pipeline. The pipeline consists of the steps represented by green ellipses and the outputs of each step represented by blue rectangles. In this work, we focus on the steps and outputs after pre-processing.\relax }}{57}{figure.caption.26}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Representation of the image classification pipeline as a directed acyclic graph. This is an instantiation of the generalized data analytic pipeline in Fig. \ref {fig:pipeline}\relax }}{58}{figure.caption.27}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Combined algorithm selection and hyper-parameter optimization in a data analytic pipeline. The algorithms and corresponding hyper-parameters are optimized simultaneously.\relax }}{59}{figure.caption.28}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Hyper-parameter optimization in a data analytic pipeline. Each path in the pipeline is individually optimized.\relax }}{59}{figure.caption.29}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Plots of error contributions from computational steps in the pipeline. Random search (blue) follows the behavior of grid search (red), whereas, bayesian optimization does not. Hence, random search maybe used to quantify the error contributions instead of grid search.\relax }}{61}{figure.caption.30}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Plots of error contributions from algorithms in the pipeline. Random search again mirrors the trend of grid search more than bayesian optimization. Therefore random search maybe used instead of grid search for computing the contribution of error from algorithms in a path. The plots also show that it is more important to tune \textit {haralick texture features}, and \textit {random forests} than it is to tune $PCA$. \relax }}{63}{figure.caption.31}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Plots of error contributions from hyper-parameters in the pipeline. Random search again mirrors the trend of grid search more than bayesian optimization. Therefore random search maybe used instead of grid search for computing the contribution of error from hyper-parameters in a path. The plots also show that it is more important to tune the hyper-parameters \textit {Haralick distance}, and \textit {Number of estimators} than it is to tune the other hyper-parameters.\relax }}{64}{figure.caption.32}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Pipeline including naive algorithms for error propagation. Random search and bayesian optimization are both much more efficient that grid search in terms of computation time.\relax }}{64}{figure.caption.33}
