\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{harrison1995validity}
\@writefile{toc}{\contentsline {chapter}{\numberline {5.}RANDOM SEARCH BASED QUANTIFICATION OF ERROR CONTRIBUTION IN IMAGE CLASSIFICATION PIPELINES}{48}{chapter.5}}
\newlabel{chap:EP}{{5}{48}{RANDOM SEARCH BASED QUANTIFICATION OF ERROR CONTRIBUTION IN IMAGE CLASSIFICATION PIPELINES}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction and Motivation}{48}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Introduction}{48}{section.5.2}}
\newlabel{sec1}{{5.2}{48}{Introduction}{section.5.2}{}}
\citation{pedregosa2011scikit}
\citation{mierswa2006yale}
\citation{spark2016apache}
\citation{bergstra2012random}
\citation{snoek2012practical}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Representation of a machine learning pipeline\relax }}{49}{figure.caption.25}}
\newlabel{fig:pipeline}{{5.1}{49}{Representation of a machine learning pipeline\relax }{figure.caption.25}{}}
\citation{ribeiro2016model}
\citation{doshi2017towards}
\citation{bergstra2012random}
\citation{snoek2012practical}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Foundations}{51}{section.5.3}}
\newlabel{sec2}{{5.3}{51}{Foundations}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Algorithm selection and hyper-parameter optimization}{51}{subsection.5.3.1}}
\newlabel{subsec_AS_HPO}{{5.3.1}{51}{Algorithm selection and hyper-parameter optimization}{subsection.5.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1.1}Hyper-parameter optimization (HPO)}{52}{subsubsection.5.3.1.1}}
\newlabel{subsubsec_HPO}{{5.3.1.1}{52}{Hyper-parameter optimization (HPO)}{subsubsection.5.3.1.1}{}}
\newlabel{hpo}{{5.1}{52}{Hyper-parameter optimization (HPO)}{equation.5.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1.2}Combined algorithm selection and hyper-parameter optimization (CASH)}{52}{subsubsection.5.3.1.2}}
\newlabel{subsubsec_CASH}{{5.3.1.2}{52}{Combined algorithm selection and hyper-parameter optimization (CASH)}{subsubsection.5.3.1.2}{}}
\newlabel{cash}{{5.2}{52}{Combined algorithm selection and hyper-parameter optimization (CASH)}{equation.5.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Optimization methods}{52}{subsection.5.3.2}}
\citation{bergstra2012random}
\citation{hutter2011sequential}
\citation{snoek2012practical}
\citation{hutter2011sequential}
\citation{bergstra2011algorithms}
\citation{expected_improvement}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2.1}Grid search}{53}{subsubsection.5.3.2.1}}
\newlabel{subsubsec1}{{5.3.2.1}{53}{Grid search}{subsubsection.5.3.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2.2}Random search}{53}{subsubsection.5.3.2.2}}
\newlabel{subsubsec2}{{5.3.2.2}{53}{Random search}{subsubsection.5.3.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2.3}Bayesian optimization}{53}{subsubsection.5.3.2.3}}
\newlabel{subsubsec1}{{5.3.2.3}{53}{Bayesian optimization}{subsubsection.5.3.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Proposed methods}{54}{section.5.4}}
\newlabel{sec3}{{5.4}{54}{Proposed methods}{section.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Error quantification}{54}{subsection.5.4.1}}
\newlabel{subsec1}{{5.4.1}{54}{Error quantification}{subsection.5.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1.1}Quantification of error from computational steps}{54}{subsubsection.5.4.1.1}}
\newlabel{eq_step}{{5.3}{55}{Quantification of error from computational steps}{equation.5.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1.2}Quantification of error from algorithms}{55}{subsubsection.5.4.1.2}}
\newlabel{eq_alg1}{{5.4}{55}{Quantification of error from algorithms}{equation.5.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experiments and results}{55}{section.5.5}}
\newlabel{sec4}{{5.5}{55}{Experiments and results}{section.5.5}{}}
\citation{breiman2001random}
\citation{cortes1995support}
\newlabel{fig:flowchart}{{\caption@xref {fig:flowchart}{ on input line 179}}{56}{Experiments and results}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Representation of an image classification pipeline. The pipeline consists of the steps represented by green ellipses and the outputs of each step represented by blue rectangles. In this work, we focus on the steps and outputs after pre-processing.\relax }}{56}{figure.caption.26}}
\citation{deng2009imagenet}
\citation{simonyan2014very}
\citation{deng2009imagenet}
\citation{szegedy2016rethinking}
\citation{wold1987principal}
\citation{tenenbaum2000global}
\citation{breiman2001random}
\citation{cortes1995support}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Representation of the image classification pipeline as a directed acyclic graph. This is an instantiation of the generalized data analytic pipeline in Fig. \ref  {fig:pipeline}\relax }}{57}{figure.caption.27}}
\newlabel{fig:images_pipeline}{{5.3}{57}{Representation of the image classification pipeline as a directed acyclic graph. This is an instantiation of the generalized data analytic pipeline in Fig. \ref {fig:pipeline}\relax }{figure.caption.27}{}}
\newlabel{algorithms_table}{{\caption@xref {algorithms_table}{ on input line 195}}{58}{Experiments and results}{figure.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Algorithms and hyper-parameters used in the image classification pipeline. The specific algorithms and corresponding \textit  {hyperparameters} are defined in the last column\relax }}{58}{table.5.1}}
\newlabel{table:2}{{5.1}{58}{Algorithms and hyper-parameters used in the image classification pipeline. The specific algorithms and corresponding \textit {hyperparameters} are defined in the last column\relax }{table.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Optimization frameworks}{58}{subsection.5.5.1}}
\citation{bilgin2007cell}
\citation{gunduz2004cell}
\citation{chowdhury2016image}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Combined algorithm selection and hyper-parameter optimization in a data analytic pipeline. The algorithms and corresponding hyper-parameters are optimized simultaneously.\relax }}{59}{figure.caption.28}}
\newlabel{fig:CASH}{{5.4}{59}{Combined algorithm selection and hyper-parameter optimization in a data analytic pipeline. The algorithms and corresponding hyper-parameters are optimized simultaneously.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Hyper-parameter optimization in a data analytic pipeline. Each path in the pipeline is individually optimized.\relax }}{59}{figure.caption.29}}
\newlabel{fig:HPO}{{5.5}{59}{Hyper-parameter optimization in a data analytic pipeline. Each path in the pipeline is individually optimized.\relax }{figure.caption.29}{}}
\citation{bilgin2007cell}
\citation{gunduz2004cell}
\citation{chowdhury2016image}
\citation{chowdhury2016image}
\citation{deng2009imagenet}
\citation{shin2016deep}
\citation{simonyan2014very}
\citation{szegedy2016rethinking}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Datasets}{60}{subsection.5.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Notations for error definitions used in the error propagation model\relax }}{60}{table.5.2}}
\newlabel{table:1}{{5.2}{60}{Notations for error definitions used in the error propagation model\relax }{table.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Error quantification experiments}{60}{subsection.5.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3.1}Error contribution from computational steps}{61}{subsubsection.5.5.3.1}}
\newlabel{fig:sfig1}{{5.6a}{62}{\textit {breast}\relax }{figure.caption.30}{}}
\newlabel{sub@fig:sfig1}{{a}{62}{\textit {breast}\relax }{figure.caption.30}{}}
\newlabel{fig:sfig2}{{5.6b}{62}{\textit {brain}\relax }{figure.caption.30}{}}
\newlabel{sub@fig:sfig2}{{b}{62}{\textit {brain}\relax }{figure.caption.30}{}}
\newlabel{fig:sfig3}{{5.6c}{62}{\textit {matsc1}\relax }{figure.caption.30}{}}
\newlabel{sub@fig:sfig3}{{c}{62}{\textit {matsc1}\relax }{figure.caption.30}{}}
\newlabel{fig:sfig4}{{5.6d}{62}{\textit {matsc2}\relax }{figure.caption.30}{}}
\newlabel{sub@fig:sfig4}{{d}{62}{\textit {matsc2}\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Plots of error contributions from computational steps in the pipeline. Random search (blue) follows the behavior of grid search (red), whereas, bayesin optimization does not. Hence, random search maybe used to quantify the error contributions instead of grid search.\relax }}{62}{figure.caption.30}}
\newlabel{fig:fig}{{5.6}{62}{Plots of error contributions from computational steps in the pipeline. Random search (blue) follows the behavior of grid search (red), whereas, bayesin optimization does not. Hence, random search maybe used to quantify the error contributions instead of grid search.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3.2}Error contribution from algorithms}{63}{subsubsection.5.5.3.2}}
\newlabel{fig:sfig1}{{5.7a}{64}{\textit {breast}\relax }{figure.caption.31}{}}
\newlabel{sub@fig:sfig1}{{a}{64}{\textit {breast}\relax }{figure.caption.31}{}}
\newlabel{fig:sfig2}{{5.7b}{64}{\textit {brain}\relax }{figure.caption.31}{}}
\newlabel{sub@fig:sfig2}{{b}{64}{\textit {brain}\relax }{figure.caption.31}{}}
\newlabel{fig:sfig3}{{5.7c}{64}{\textit {matsc1}\relax }{figure.caption.31}{}}
\newlabel{sub@fig:sfig3}{{c}{64}{\textit {matsc1}\relax }{figure.caption.31}{}}
\newlabel{fig:sfig4}{{5.7d}{64}{\textit {matsc2}\relax }{figure.caption.31}{}}
\newlabel{sub@fig:sfig4}{{d}{64}{\textit {matsc2}\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Plots of error contributions from algorithms in the pipeline. Random search again mirrors the trend of grid search more than bayesian optimization. Therefore random search maybe used instead of grid search for computing the contribution of error from algorithms in a path.\relax }}{64}{figure.caption.31}}
\newlabel{fig:fig}{{5.7}{64}{Plots of error contributions from algorithms in the pipeline. Random search again mirrors the trend of grid search more than bayesian optimization. Therefore random search maybe used instead of grid search for computing the contribution of error from algorithms in a path.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Pipeline including naive algorithms for error propagation. Random search and bayesian optimization are both much more efficient that grid search in terms of computation time.\relax }}{65}{figure.caption.32}}
\newlabel{fig:flowchart}{{5.8}{65}{Pipeline including naive algorithms for error propagation. Random search and bayesian optimization are both much more efficient that grid search in terms of computation time.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Conclusion}{65}{section.5.6}}
\newlabel{sec5}{{5.6}{66}{Conclusion}{section.5.6}{}}
\@setckpt{EP}{
\setcounter{page}{67}
\setcounter{equation}{4}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{2}
\setcounter{firstchapter}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{parentequation}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{70}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{algorithm}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{prop}{0}
\setcounter{rmk}{0}
\setcounter{section@level}{1}
\setcounter{lstlisting}{0}
}
