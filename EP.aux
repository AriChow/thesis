\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{harrison1995validity}
\citation{pedregosa2011scikit}
\citation{mierswa2006yale}
\citation{spark2016apache}
\citation{bergstra2012random}
\citation{snoek2012practical,zhang2016flash}
\citation{olson2016evaluation,olson2016tpot,olson2016automating}
\@writefile{toc}{\contentsline {chapter}{\numberline {5.}ALGORITHM SELECTION AND HYPERPARAMETER OPTIMIZATION BASED QUANTIFICATION OF ERROR CONTRIBUTION IN IMAGE CLASSIFICATION PIPELINES}{48}{chapter.5}}
\newlabel{chap:EP}{{5}{48}{ALGORITHM SELECTION AND HYPERPARAMETER OPTIMIZATION BASED QUANTIFICATION OF ERROR CONTRIBUTION IN IMAGE CLASSIFICATION PIPELINES}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{48}{section.5.1}}
\newlabel{sec1}{{5.1}{48}{Introduction}{section.5.1}{}}
\citation{ribeiro2016model}
\citation{doshi2017towards}
\citation{koh2017understanding}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Representation of a machine learning pipeline\relax }}{49}{figure.caption.25}}
\newlabel{fig:pipeline}{{5.1}{49}{Representation of a machine learning pipeline\relax }{figure.caption.25}{}}
\citation{bergstra2012random}
\citation{snoek2012practical}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Foundations}{51}{section.5.2}}
\newlabel{sec2}{{5.2}{51}{Foundations}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Algorithm selection and hyper-parameter optimization}{51}{subsection.5.2.1}}
\newlabel{subsec_AS_HPO}{{5.2.1}{51}{Algorithm selection and hyper-parameter optimization}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1.1}Hyper-parameter optimization (HPO)}{51}{subsubsection.5.2.1.1}}
\newlabel{subsubsec_HPO}{{5.2.1.1}{51}{Hyper-parameter optimization (HPO)}{subsubsection.5.2.1.1}{}}
\newlabel{hpo}{{5.1}{51}{Hyper-parameter optimization (HPO)}{equation.5.2.1}{}}
\citation{bergstra2012random}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1.2}Combined algorithm selection and hyper-parameter optimization (CASH)}{52}{subsubsection.5.2.1.2}}
\newlabel{subsubsec_CASH}{{5.2.1.2}{52}{Combined algorithm selection and hyper-parameter optimization (CASH)}{subsubsection.5.2.1.2}{}}
\newlabel{cash}{{5.2}{52}{Combined algorithm selection and hyper-parameter optimization (CASH)}{equation.5.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Optimization methods}{52}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2.1}Grid search}{52}{subsubsection.5.2.2.1}}
\newlabel{subsubsec1}{{5.2.2.1}{52}{Grid search}{subsubsection.5.2.2.1}{}}
\citation{hutter2011sequential}
\citation{snoek2012practical}
\citation{hutter2011sequential}
\citation{bergstra2011algorithms}
\citation{expected_improvement}
\citation{eggensperger2013towards}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2.2}Random search}{53}{subsubsection.5.2.2.2}}
\newlabel{subsubsec2}{{5.2.2.2}{53}{Random search}{subsubsection.5.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2.3}Bayesian optimization}{53}{subsubsection.5.2.2.3}}
\newlabel{subsubsec1}{{5.2.2.3}{53}{Bayesian optimization}{subsubsection.5.2.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Proposed methods}{53}{section.5.3}}
\newlabel{sec3}{{5.3}{53}{Proposed methods}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Error contribution with the agnostic methodology}{53}{subsection.5.3.1}}
\newlabel{EQ}{{5.3.1}{53}{Error contribution with the agnostic methodology}{subsection.5.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1.1}Quantification of error from computational steps}{54}{subsubsection.5.3.1.1}}
\newlabel{subsubsec_eq_steps}{{5.3.1.1}{54}{Quantification of error from computational steps}{subsubsection.5.3.1.1}{}}
\newlabel{eq_step}{{5.3}{54}{Quantification of error from computational steps}{equation.5.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1.2}Quantification of error from algorithms}{55}{subsubsection.5.3.1.2}}
\newlabel{subsubsec_eq_alg}{{5.3.1.2}{55}{Quantification of error from algorithms}{subsubsection.5.3.1.2}{}}
\newlabel{eq_alg}{{5.4}{55}{Quantification of error from algorithms}{equation.5.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1.3}Quantification of error from hyper-parameters}{55}{subsubsection.5.3.1.3}}
\newlabel{subsubsec_eq_hyper}{{5.3.1.3}{55}{Quantification of error from hyper-parameters}{subsubsection.5.3.1.3}{}}
\citation{breiman2001random}
\citation{cortes1995support}
\citation{deng2009imagenet}
\citation{simonyan2014very}
\citation{deng2009imagenet}
\citation{szegedy2016rethinking}
\citation{wold1987principal}
\citation{tenenbaum2000global}
\citation{breiman2001random}
\citation{cortes1995support}
\newlabel{eq_hyper}{{5.5}{56}{Quantification of error from hyper-parameters}{equation.5.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiments and results}{56}{section.5.4}}
\newlabel{sec4}{{5.4}{56}{Experiments and results}{section.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Representation of an image classification pipeline. The pipeline consists of the steps represented by green ellipses and the outputs of each step represented by blue rectangles. In this work, we focus on the steps and outputs after pre-processing.\relax }}{57}{figure.caption.26}}
\newlabel{fig:flowchart}{{5.2}{57}{Representation of an image classification pipeline. The pipeline consists of the steps represented by green ellipses and the outputs of each step represented by blue rectangles. In this work, we focus on the steps and outputs after pre-processing.\relax }{figure.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Algorithms and hyper-parameters used in the image classification pipeline. The specific algorithms and corresponding \textit  {hyperparameters} are defined in the last column\relax }}{57}{table.5.1}}
\newlabel{table:algorithms_table}{{5.1}{57}{Algorithms and hyper-parameters used in the image classification pipeline. The specific algorithms and corresponding \textit {hyperparameters} are defined in the last column\relax }{table.5.1}{}}
\citation{bilgin2007cell}
\citation{gunduz2004cell}
\citation{chowdhury2016image}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Representation of the image classification pipeline as a directed acyclic graph. This is an instantiation of the generalized data analytic pipeline in Fig. \ref  {fig:pipeline}\relax }}{58}{figure.caption.27}}
\newlabel{fig:images_pipeline}{{5.3}{58}{Representation of the image classification pipeline as a directed acyclic graph. This is an instantiation of the generalized data analytic pipeline in Fig. \ref {fig:pipeline}\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Optimization frameworks}{58}{subsection.5.4.1}}
\newlabel{frameworks}{{5.4.1}{58}{Optimization frameworks}{subsection.5.4.1}{}}
\citation{bilgin2007cell}
\citation{gunduz2004cell}
\citation{chowdhury2016image}
\citation{chowdhury2016image}
\citation{deng2009imagenet}
\citation{shin2016deep}
\citation{simonyan2014very}
\citation{szegedy2016rethinking}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Combined algorithm selection and hyper-parameter optimization in a data analytic pipeline. The algorithms and corresponding hyper-parameters are optimized simultaneously.\relax }}{59}{figure.caption.28}}
\newlabel{fig:CASH}{{5.4}{59}{Combined algorithm selection and hyper-parameter optimization in a data analytic pipeline. The algorithms and corresponding hyper-parameters are optimized simultaneously.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Hyper-parameter optimization in a data analytic pipeline. Each path in the pipeline is individually optimized.\relax }}{59}{figure.caption.29}}
\newlabel{fig:HPO}{{5.5}{59}{Hyper-parameter optimization in a data analytic pipeline. Each path in the pipeline is individually optimized.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Datasets}{59}{subsection.5.4.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Notations for error definitions used in the error propagation model\relax }}{60}{table.5.2}}
\newlabel{table:datasets}{{5.2}{60}{Notations for error definitions used in the error propagation model\relax }{table.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Error quantification experiments}{60}{subsection.5.4.3}}
\newlabel{eq_expts}{{5.4.3}{60}{Error quantification experiments}{subsection.5.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3.1}Error contribution from computational steps}{60}{subsubsection.5.4.3.1}}
\newlabel{fig:eq_steps_breast}{{5.6a}{61}{\textit {breast}\relax }{figure.caption.30}{}}
\newlabel{sub@fig:eq_steps_breast}{{a}{61}{\textit {breast}\relax }{figure.caption.30}{}}
\newlabel{fig:eq_step_brain}{{5.6b}{61}{\textit {brain}\relax }{figure.caption.30}{}}
\newlabel{sub@fig:eq_step_brain}{{b}{61}{\textit {brain}\relax }{figure.caption.30}{}}
\newlabel{fig:eq_steps_matsc1}{{5.6c}{61}{\textit {matsc1}\relax }{figure.caption.30}{}}
\newlabel{sub@fig:eq_steps_matsc1}{{c}{61}{\textit {matsc1}\relax }{figure.caption.30}{}}
\newlabel{fig:eq_steps_matsc2}{{5.6d}{61}{\textit {matsc2}\relax }{figure.caption.30}{}}
\newlabel{sub@fig:eq_steps_matsc2}{{d}{61}{\textit {matsc2}\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Plots of error contributions from computational steps in the pipeline. Random search (blue) follows the behavior of grid search (red), whereas, bayesian optimization does not. Hence, random search maybe used to quantify the error contributions instead of grid search.\relax }}{61}{figure.caption.30}}
\newlabel{fig:eq_steps}{{5.6}{61}{Plots of error contributions from computational steps in the pipeline. Random search (blue) follows the behavior of grid search (red), whereas, bayesian optimization does not. Hence, random search maybe used to quantify the error contributions instead of grid search.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3.2}Error contribution from algorithms}{62}{subsubsection.5.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3.3}Error contribution from hyper-parameters}{62}{subsubsection.5.4.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3.4}Comparison of computation time}{62}{subsubsection.5.4.3.4}}
\newlabel{fig:sfig1}{{5.7a}{63}{\textit {breast}\relax }{figure.caption.31}{}}
\newlabel{sub@fig:sfig1}{{a}{63}{\textit {breast}\relax }{figure.caption.31}{}}
\newlabel{fig:sfig2}{{5.7b}{63}{\textit {brain}\relax }{figure.caption.31}{}}
\newlabel{sub@fig:sfig2}{{b}{63}{\textit {brain}\relax }{figure.caption.31}{}}
\newlabel{fig:sfig3}{{5.7c}{63}{\textit {matsc1}\relax }{figure.caption.31}{}}
\newlabel{sub@fig:sfig3}{{c}{63}{\textit {matsc1}\relax }{figure.caption.31}{}}
\newlabel{fig:sfig4}{{5.7d}{63}{\textit {matsc2}\relax }{figure.caption.31}{}}
\newlabel{sub@fig:sfig4}{{d}{63}{\textit {matsc2}\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Plots of error contributions from algorithms in the pipeline. Random search again mirrors the trend of grid search more than bayesian optimization. Therefore random search maybe used instead of grid search for computing the contribution of error from algorithms in a path. The plots also show that it is more important to tune \textit  {haralick texture features}, and \textit  {random forests} than it is to tune $PCA$. \relax }}{63}{figure.caption.31}}
\newlabel{fig:fig}{{5.7}{63}{Plots of error contributions from algorithms in the pipeline. Random search again mirrors the trend of grid search more than bayesian optimization. Therefore random search maybe used instead of grid search for computing the contribution of error from algorithms in a path. The plots also show that it is more important to tune \textit {haralick texture features}, and \textit {random forests} than it is to tune $PCA$. \relax }{figure.caption.31}{}}
\newlabel{fig:sfig1}{{5.8a}{64}{\textit {breast}\relax }{figure.caption.32}{}}
\newlabel{sub@fig:sfig1}{{a}{64}{\textit {breast}\relax }{figure.caption.32}{}}
\newlabel{fig:sfig2}{{5.8b}{64}{\textit {brain}\relax }{figure.caption.32}{}}
\newlabel{sub@fig:sfig2}{{b}{64}{\textit {brain}\relax }{figure.caption.32}{}}
\newlabel{fig:sfig3}{{5.8c}{64}{\textit {matsc1}\relax }{figure.caption.32}{}}
\newlabel{sub@fig:sfig3}{{c}{64}{\textit {matsc1}\relax }{figure.caption.32}{}}
\newlabel{fig:sfig4}{{5.8d}{64}{\textit {matsc2}\relax }{figure.caption.32}{}}
\newlabel{sub@fig:sfig4}{{d}{64}{\textit {matsc2}\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Plots of error contributions from hyper-parameters in the pipeline. Random search again mirrors the trend of grid search more than bayesian optimization. Therefore random search maybe used instead of grid search for computing the contribution of error from hyper-parameters in a path. The plots also show that it is more important to tune the hyper-parameters \textit  {Haralick distance}, and \textit  {Number of estimators} than it is to tune the other hyper-parameters.\relax }}{64}{figure.caption.32}}
\newlabel{fig:eq_hyper}{{5.8}{64}{Plots of error contributions from hyper-parameters in the pipeline. Random search again mirrors the trend of grid search more than bayesian optimization. Therefore random search maybe used instead of grid search for computing the contribution of error from hyper-parameters in a path. The plots also show that it is more important to tune the hyper-parameters \textit {Haralick distance}, and \textit {Number of estimators} than it is to tune the other hyper-parameters.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Pipeline including naive algorithms for error propagation. Random search and bayesian optimization are both much more efficient that grid search in terms of computation time.\relax }}{64}{figure.caption.33}}
\newlabel{fig:time}{{5.9}{64}{Pipeline including naive algorithms for error propagation. Random search and bayesian optimization are both much more efficient that grid search in terms of computation time.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusion}{65}{section.5.5}}
\newlabel{sec5}{{5.5}{65}{Conclusion}{section.5.5}{}}
\@setckpt{EP}{
\setcounter{page}{66}
\setcounter{equation}{5}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{2}
\setcounter{firstchapter}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{parentequation}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{72}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{algorithm}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstnumber}{1}
\setcounter{prop}{0}
\setcounter{rmk}{0}
\setcounter{section@level}{1}
\setcounter{lstlisting}{0}
}
